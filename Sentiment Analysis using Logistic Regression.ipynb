{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Logistic Regression\n",
    "\n",
    "Case of study: Indonesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import nltk\n",
    "import nltk\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "# this enables importing of these files without downloading it again when we refresh our workspace\n",
    "\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "from utils_ind import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yuk sini kita musuhan biar aku teror km :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lu mah sukanga om om :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kannnn :( target a tapi bila dah macam ni dah ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tp klopun msk gw pribadipun blm yakin bisa non...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 personil mejile besok mau pulang tiati ya ka...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>simple aja sih. jangan mainin perasaan cewek, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>telkomsel: mhmdaliwafa jika log in instagram d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>telkomsel: lanwsp [3] info lengkap friday movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>halobca: rizkyrinjani_ mhn bpk rizky infokan n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>pokoknya itu ngedesah titik kgk pake koma:) de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0           yuk sini kita musuhan biar aku teror km :(     -1\n",
       "1                              lu mah sukanga om om :(     -1\n",
       "2    kannnn :( target a tapi bila dah macam ni dah ...     -1\n",
       "3    tp klopun msk gw pribadipun blm yakin bisa non...     -1\n",
       "4    2 personil mejile besok mau pulang tiati ya ka...     -1\n",
       "..                                                 ...    ...\n",
       "195  simple aja sih. jangan mainin perasaan cewek, ...      1\n",
       "196  telkomsel: mhmdaliwafa jika log in instagram d...      1\n",
       "197  telkomsel: lanwsp [3] info lengkap friday movi...      1\n",
       "198  halobca: rizkyrinjani_ mhn bpk rizky infokan n...      1\n",
       "199  pokoknya itu ngedesah titik kgk pake koma:) de...      1\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"sentiment_sample.xlsx\")\n",
    "df = df.drop(columns=['id'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = df[df['label'] == 1].values\n",
    "all_negative_tweets = df[df['label'] == -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = all_positive_tweets[80:,0]\n",
    "train_pos = all_positive_tweets[:80,0]\n",
    "test_neg = all_negative_tweets[80:,0]\n",
    "train_neg = all_negative_tweets[:80,0]\n",
    "\n",
    "train_x = np.hstack((train_pos,train_neg))\n",
    "test_x = np.hstack((test_pos,test_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (160, 1)\n",
      "test_y.shape = (40, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 593\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation, remove stopwords, stemming and cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " lagi berjuang:)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['juang', '']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    \n",
    "    # calculate the sigmoid of z\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n",
      "CORRECT!\n"
     ]
    }
   ],
   "source": [
    "# Testing function \n",
    "if (sigmoid(0) == 0.5):\n",
    "    print('SUCCESS!')\n",
    "else:\n",
    "    print('Oops!')\n",
    "\n",
    "if (sigmoid(4.92) == 0.9927537604041685):\n",
    "    print('CORRECT!')\n",
    "else:\n",
    "    print('Oops again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent and Cost Function\n",
    "\n",
    "this section i will use Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x,theta)\n",
    "        \n",
    "        # get the sigmoid of h\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        # note that we can use also np.array.transpose() instead of np.array.T\n",
    "        # np.array.T just makes code a little more readable :)\n",
    "        J = -1./m * (np.dot(y.T, np.log(h)) + np.dot((1-y).T,np.log(1-h)))                                                    \n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha/m) * np.dot(x.T,(h-y))\n",
    "        \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.67094970.\n",
      "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
     ]
    }
   ],
   "source": [
    "# Check the function\n",
    "# Construct a synthetic test case using numpy PRNG functions\n",
    "np.random.seed(1)\n",
    "# X input is 10 x 3 with ones for the bias terms\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "# Y Labels are 10 x 1\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "# Apply gradient descent\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
    "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1.0),0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0.0),0)\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. 97. 88.]]\n"
     ]
    }
   ],
   "source": [
    "# Check your function\n",
    "\n",
    "# test 1\n",
    "# test on training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test 2:\n",
    "# check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.69226653.\n",
      "The resulting vector of weights is [-0.0, 3.636e-05, -3.2e-07]\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet,freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saya senang -> 0.500000\n",
      "Saya sedih -> 0.500000\n",
      "Film ini bagus -> 0.500000\n",
      "Luar biasa -> 0.500000\n",
      "Bagus -> 0.500000\n",
      "Bagus sekali -> 0.500000\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['Saya senang', 'Saya sedih', 'Film ini bagus', 'Luar biasa', 'Bagus', 'Bagus sekali']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50086538]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "# Feel free to check the sentiment of your own tweet below\n",
    "\n",
    "my_tweet = 'Saya pandai :)'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    \n",
    "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.5000\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "NO ERROR\n",
      "THE TWEET IS: bom akutuh sibuk bom sbl :(\n",
      "THE PROCESSED TWEET IS: ['bom', 'akutuh', 'sibuk', 'bom', 'sbl', '']\n",
      "0\t0.50086538\tb'bom akutuh sibuk bom sbl '\n",
      "THE TWEET IS: yah sedih :(\n",
      "THE PROCESSED TWEET IS: ['yah', 'sedih', '']\n",
      "0\t0.50086498\tb'yah sedih '\n",
      "THE TWEET IS: yu  , maap yah kmrn ga bsa dateng :( maap yaaahh. nasi tumpeng msh ada?? sarapan yes, martabak keju nih enak\n",
      "THE PROCESSED TWEET IS: ['yu', 'maap', 'yah', 'kmrn', 'ga', 'bsa', 'dateng', '', 'maap', 'yaaahh', 'nasi', 'tumpeng', 'msh', 'sarap', 'yes', 'martabak', 'keju', 'nih', 'enak']\n",
      "0\t0.50087334\tb'yu maap yah kmrn ga bsa dateng  maap yaaahh nasi tumpeng msh sarap yes martabak keju nih enak'\n",
      "THE TWEET IS: sabar banget ya lu hyung :(\n",
      "THE PROCESSED TWEET IS: ['sabar', 'banget', 'ya', 'lu', 'hyung', '']\n",
      "0\t0.50090068\tb'sabar banget ya lu hyung '\n",
      "THE TWEET IS: nurhadi mah punya oppa:(\n",
      "THE PROCESSED TWEET IS: ['nurhadi', 'mah', 'oppa', '']\n",
      "0\t0.50086473\tb'nurhadi mah oppa '\n",
      "THE TWEET IS: harga app, musik, film di itunes store amrik beda sama yg indonesia. akun gue amrik, harganya hampir 2x lipat. :((\n",
      "THE PROCESSED TWEET IS: ['harga', 'app', 'musik', 'film', 'itunes', 'store', 'amrik', 'beda', 'yg', 'indonesia', 'akun', 'gue', 'amrik', 'harga', '2x', 'lipat', '']\n",
      "0\t0.50093753\tb'harga app musik film itunes store amrik beda yg indonesia akun gue amrik harga 2x lipat '\n",
      "THE TWEET IS: kpn kmbali??. kpn dtg nya \"kn indh pd wktunya\"??. pengen ky dlu lagiii :(\n",
      "THE PROCESSED TWEET IS: ['kpn', 'kmbali', 'kpn', 'dtg', 'nya', 'kn', 'indh', 'pd', 'wktunya', 'ken', 'ky', 'dlu', 'lagiii', '']\n",
      "0\t0.50088356\tb'kpn kmbali kpn dtg nya kn indh pd wktunya ken ky dlu lagiii '\n",
      "THE TWEET IS: pacaran: nungguin pacar jemput   ldr: nungguin di telpon   jomblo: nungguin di usir. :(\n",
      "THE PROCESSED TWEET IS: ['pacar', 'nungguin', 'pacar', 'jemput', 'ldr', 'nungguin', 'telpon', 'jomblo', 'nungguin', 'usir', '']\n",
      "0\t0.50086530\tb'pacar nungguin pacar jemput ldr nungguin telpon jomblo nungguin usir '\n",
      "THE TWEET IS: yaudah gausah percaya sm kamu :(\n",
      "THE PROCESSED TWEET IS: ['yaudah', 'gausah', 'percaya', 'sm', '']\n",
      "0\t0.50086514\tb'yaudah gausah percaya sm '\n",
      "THE TWEET IS: rindu buddy more :( bila nak jumpa awak?\n",
      "THE PROCESSED TWEET IS: ['rindu', 'buddy', 'more', '', 'nak', 'jumpa', 'awak']\n",
      "0\t0.50086530\tb'rindu buddy more  nak jumpa awak'\n",
      "THE TWEET IS: ...... kenapasih aiko, kenapa yanglek. :(\n",
      "THE PROCESSED TWEET IS: ['', 'kenapasih', 'aiko', 'yanglek', '']\n",
      "0\t0.50173076\tb' kenapasih aiko yanglek '\n",
      "THE TWEET IS: tapi hati u dah ada orang lain :(\n",
      "THE PROCESSED TWEET IS: ['hati', 'u', 'dah', 'orang', '']\n",
      "0\t0.50086498\tb'hati u dah orang '\n",
      "THE TWEET IS: maah, aku di perkosa si anu, aku kotor maaah :(   tenang, berani kotor itu baik, kan ada rinso~   -___-\n",
      "THE PROCESSED TWEET IS: ['maah', 'perkosa', 'si', 'anu', 'kotor', 'maaah', '', 'tenang', 'berani', 'kotor', 'rinso', '']\n",
      "0\t0.50173052\tb'maah perkosa si anu kotor maaah  tenang berani kotor rinso '\n",
      "THE TWEET IS: itumah cerita jaman dulu ayang :(\n",
      "THE PROCESSED TWEET IS: ['itumah', 'cerita', 'jaman', 'ayang', '']\n",
      "0\t0.50086530\tb'itumah cerita jaman ayang '\n",
      "THE TWEET IS: aku mau nangis:(\n",
      "THE PROCESSED TWEET IS: ['nang', '']\n",
      "0\t0.50086530\tb'nang '\n",
      "THE TWEET IS: kasian:(\n",
      "THE PROCESSED TWEET IS: ['kasi', '']\n",
      "0\t0.50088356\tb'kasi '\n",
      "THE TWEET IS: (( masa nda kenal :(\n",
      "THE PROCESSED TWEET IS: ['nda', 'kenal', '']\n",
      "0\t0.50087447\tb'nda kenal '\n",
      "THE TWEET IS: jan homoan di mentab cheonsa, jebal:((\n",
      "THE PROCESSED TWEET IS: ['jan', 'homo', 'mentab', 'cheonsa', 'jebal', '']\n",
      "0\t0.50086538\tb'jan homo mentab cheonsa jebal '\n",
      "THE TWEET IS: huhu makin penasaran cobaa, kesel bgt tiap chapter updatenya dikit bgt :(\n",
      "THE PROCESSED TWEET IS: ['huhu', 'penasaran', 'cobaa', 'kesel', 'bgt', 'chapter', 'updatenya', 'dikit', 'bgt', '']\n",
      "0\t0.50086482\tb'huhu penasaran cobaa kesel bgt chapter updatenya dikit bgt '\n",
      "THE TWEET IS: itu ava jaman masih kurus, jaman undebut :(\n",
      "THE PROCESSED TWEET IS: ['ava', 'jaman', 'kurus', 'jaman', 'undebut', '']\n",
      "0\t0.50086522\tb'ava jaman kurus jaman undebut '\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n",
    "    else:\n",
    "        print('NO ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test by using New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['senang', 'ajar', 'bahasa']\n",
      "[[0.5]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change the tweet below\n",
    "my_tweet = 'Saya tidak senang belajar bahasa'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
